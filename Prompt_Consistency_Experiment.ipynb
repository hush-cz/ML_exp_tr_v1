{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1QsT9VE81OgkuPabtlbeEHvzpuFdE7rY6",
      "authorship_tag": "ABX9TyMSCG3mGN/hFTJIjEKT8GA9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hush-cz/ML_exp_tr_v1/blob/main/Prompt_Consistency_Experiment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8rjg3R_ulpxG"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "0p8mSQrErMg0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prompt Consistency Experiment\n",
        "\n",
        "**Author:** Tomáš Vodáček\n",
        "**Goal:** Ověřit, jak konzistentně odpovídá jazykový model (LLM) na faktické otázky při různém promptu.\n",
        "**Method:** Použití open-source modelu z HuggingFace (DistilGPT-2).  \n",
        "**Expected outcome:** Vyhodnocení, zda model odpovídá konzistentně, nebo vykazuje variabilitu.  \n",
        "**Instrument:** Iphone and poor Malta wifi :)"
      ],
      "metadata": {
        "id": "XFKz1jM6rMrR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "Z9LMb_HpryJ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline, set_seed\n",
        "\n",
        "# inicializace generátoru textu\n",
        "generator = pipeline(\"text-generation\", model=\"distilgpt2\")\n",
        "set_seed(42)"
      ],
      "metadata": {
        "id": "LanGc-J4r_lh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompts = [\n",
        "    \"What is the capital of France?\",\n",
        "    \"France's capital city is ...\",\n",
        "    \"Name the capital of France.\",\n",
        "    \"Which city is the capital of France?\",\n",
        "    \"Give me the capital of France.\"\n",
        "]"
      ],
      "metadata": {
        "id": "wN2tEH0staaI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = {}\n",
        "\n",
        "for prompt in prompts:\n",
        "    outputs = generator(prompt, max_length=20, num_return_sequences=3)\n",
        "    results[prompt] = [out['generated_text'] for out in outputs]\n",
        "\n",
        "results"
      ],
      "metadata": {
        "id": "KqHd_jpXtgF_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for prompt, outputs in results.items():\n",
        "    print(f\"\\nPrompt: {prompt}\")\n",
        "    for o in outputs:\n",
        "        print(\"  ->\", o.replace(\"\\n\",\" \"))"
      ],
      "metadata": {
        "id": "ygMW4G7euJqV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "correct_count = 0\n",
        "total_count = 0\n",
        "\n",
        "for prompt, outputs in results.items():\n",
        "    for o in outputs:\n",
        "        total_count += 1\n",
        "        if \"Paris\" in o:\n",
        "            correct_count += 1\n",
        "\n",
        "accuracy = correct_count / total_count * 100\n",
        "print(f\"Consistency accuracy: {accuracy:.2f}% ({correct_count}/{total_count} answers contained 'Paris')\")"
      ],
      "metadata": {
        "id": "5geHsLndutdD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "This simple experiment demonstrated that a small language model (DistilGPT-2)\n",
        "was able to produce the correct answer (\"Paris\") only 2 out of 15 times\n",
        "(~13% consistency) when asked the same fact-based question in different ways.\n",
        "\n",
        "This highlights two important points:\n",
        "1. Smaller open-source models often lack factual robustness.\n",
        "2. Evaluating consistency across prompt variations is crucial for measuring reliability.\n",
        "\n",
        "A natural next step would be to repeat this experiment with larger models\n",
        "(e.g., GPT-Neo, LLaMA) and compare the consistency rates.\n",
        "\n"
      ],
      "metadata": {
        "id": "LWqpyra2vgI5"
      }
    }
  ]
}